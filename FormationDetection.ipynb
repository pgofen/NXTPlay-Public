{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO8LAJ5w4um/54pVH76+rI8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/William-Metz/NXTPlaySampleNotebooks/blob/main/FormationDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In this notebook import the data set with player coorindates and formations, uses keras to try to predict the formation based on the coordinates"
      ],
      "metadata": {
        "id": "Z6DhofnzoQt7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation and Setup\n"
      ],
      "metadata": {
        "id": "9wnZoEZ51ETL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner\n",
        "!pip install tensorflow-gpu\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.regularizers import l1\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from keras_tuner.tuners import RandomSearch\n",
        "!pip install keras-tuner\n",
        "!pip install tensorflow-gpu\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
      ],
      "metadata": {
        "id": "yCiszfoHlA0B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b28b0b65-c820-4318-cfc0-5bda8c283da9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.6-py3-none-any.whl (128 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/128.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/128.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.9/128.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (23.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2023.11.17)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.6 kt-legacy-1.0.5\n",
            "Collecting tensorflow-gpu\n",
            "  Downloading tensorflow-gpu-2.12.0.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.10/dist-packages (1.4.6)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (23.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2023.11.17)\n",
            "Collecting tensorflow-gpu\n",
            "  Using cached tensorflow-gpu-2.12.0.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Num GPUs Available:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Wnhpurvlt9h",
        "outputId": "73a2373c-acb2-4a41-ceaf-30072a179dea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Data\n",
        "In order to make this notebook work, make sure to have the dataset with player coordinates\n"
      ],
      "metadata": {
        "id": "XOu-OCxF1K5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/MidtermProject/simplifiedDataWithCoordinates.csv')\n",
        "\n",
        "columns_to_keep = [f'offense_player{i}_x' for i in range(1, 12)] + \\\n",
        "                  [f'offense_player{i}_y' for i in range(1, 12)] +\\\n",
        "                  [f'defense_player{i}_x' for i in range(1, 12)] + \\\n",
        "                  [f'defense_player{i}_y' for i in range(1, 12)] + \\\n",
        "                   ['off_formation']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df = df[columns_to_keep]\n",
        "\n",
        "# Replace 0.0 with numpy.nan\n",
        "df = df.replace(0.0, np.nan)\n",
        "\n",
        "# Drop rows with NaN in specific columns\n",
        "#df = df.dropna(subset=['offense_player1_x', 'defense_player2_x', 'defense_player2_x', 'defense_player1_x'])\n",
        "#Drops empty players\n",
        "df = df.dropna(subset=['offense_player7_x', 'defense_player7_x'])\n",
        "# Replace NaN back with 0.0 if necessary\n",
        "df = df.fillna(0.0)\n"
      ],
      "metadata": {
        "id": "oahH7WB-dvvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This was done unifomize the number of each formation in the dataset."
      ],
      "metadata": {
        "id": "KsJ3F2DZolVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dic = {}\n",
        "for index, row in df.iterrows():\n",
        "    # access data using row['column_name']\n",
        "    dic[row['off_formation']] = dic.get(row['off_formation'], 0) + 1\n",
        "print(df.shape)\n",
        "print(dic)\n",
        "for key, value in dic.items():\n",
        "    if value < 20:\n",
        "        # Drop rows where 'off_formation' equals key\n",
        "        df.drop(df[df['off_formation'] == key].index, inplace=True)\n",
        "    else:\n",
        "        # Get indices where 'off_formation' equals key\n",
        "        indices = df[df['off_formation'] == key].index\n",
        "        print(len(indices))\n",
        "        # Randomly select 19 or fewer indices from this list\n",
        "        indices_to_drop = np.random.choice(indices, size= len(indices) -20 , replace=False)\n",
        "\n",
        "        # Drop the selected rows\n",
        "        df.drop(indices_to_drop, inplace=True)\n",
        "print(len(dic))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVcwUV1yd4X-",
        "outputId": "32f4cc19-e6de-464f-a875-5d24ce1aacc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(372, 45)\n",
            "{'B FLEX': 21, 'LIGHTNING': 25, 'CHIEF': 6, 'KINGS SPLIT RIGHT ': 73, 'KINGS SPLIT': 11, 'KINGS TIGHT': 3, 'SPACE': 2, 'SPLIT': 13, 'SPLIT FLEX': 2, 'LIGHTNING SQUEEZE': 3, 'PLANE FLEX': 9, 'BENGALS': 6, 'STRONG SPLIT': 1, 'STRONG FLEX': 1, 'CHARGER': 2, 'CINCINNATI': 1, 'KINGS SPLIT LEFT': 60, 'LIGHTNING STACK': 2, 'EMPTY': 14, 'LIGHTNING TITE LEFT': 31, 'UNBALANCED ': 1, 'TWINS SPLIT LEFT': 6, 'ACES LEFT ': 4, 'LIGHTNING TITE RIGHT': 35, 'TWINS SPLIT RIGHT': 7, 'QUEENS LEFT': 9, 'QUEENS RIGHT': 15, 'ACES Right': 4, 'KINGS RIGHT': 5}\n",
            "21\n",
            "25\n",
            "73\n",
            "60\n",
            "31\n",
            "35\n",
            "29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split into testing and training"
      ],
      "metadata": {
        "id": "gAr0MNFOo2Cz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKrEgmRFkRbA"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = pd.get_dummies(df, columns=['off_formation'])\n",
        "X = df.drop([col for col in df.columns if col.startswith('off_formation')], axis=1)\n",
        "y = df[[col for col in df.columns if col.startswith('off_formation')]]\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and testing different models"
      ],
      "metadata": {
        "id": "UiWpE5J9pOtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(hp.Int('input_units', min_value=32, max_value=256, step=32),\n",
        "                    activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
        "\n",
        "    for i in range(hp.Int('n_layers', 1, 4)):\n",
        "        model.add(Dense(hp.Int(f'dense_{i}_units', min_value=32, max_value=256, step=32),\n",
        "                        activation='relu', kernel_regularizer=l1(hp.Choice('l1_reg', values=[0.01, 0.001, 0.0001]))))\n",
        "        model.add(Dropout(hp.Float('dropout', min_value=0.2, max_value=0.5, step=0.1)))\n",
        "\n",
        "    model.add(Dense(y_train.shape[1], activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer=Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "F4lz4lWWeR-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,  # Number of trials to run\n",
        "    executions_per_trial=1,  # Number of models to train per trial\n",
        "    directory='my_dir',\n",
        "    project_name='hparam_tuning'\n",
        ")\n",
        "with tf.device('/GPU:0'):\n",
        "  tuner.search(X_train_scaled, y_train, epochs=100, validation_split=0.2)\n",
        "  best_model = tuner.get_best_models(num_models=1)[0]\n",
        "  loss, accuracy = best_model.evaluate(X_test_scaled, y_test)\n",
        "  print(f\"Test Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1ska5OlemY2",
        "outputId": "0a72756f-f695-48e0-ac2e-f755f3b57fc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10 Complete [00h 00m 06s]\n",
            "val_accuracy: 0.25\n",
            "\n",
            "Best val_accuracy So Far: 0.3499999940395355\n",
            "Total elapsed time: 00h 01m 03s\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 2.6165 - accuracy: 0.2083\n",
            "Test Accuracy: 0.2083333283662796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in X_train_scaled:\n",
        "\n",
        "  # Reshape the data for the model, if necessary\n",
        "  predictions = best_model.predict(i.reshape(1, -1))\n",
        "  predicted_class_index = np.argmax(predictions, axis=1)\n",
        "  text = y.columns[predicted_class_index][0]  # G\n",
        "  print(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajoMp2CFjxrJ",
        "outputId": "db79d0da-9edc-4f55-a6f9-fbb03580dfdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 22ms/step\n",
            "off_formation_KINGS SPLIT LEFT\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_EMPTY\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_LIGHTNING TITE LEFT\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "off_formation_KINGS SPLIT RIGHT \n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "off_formation_LIGHTNING TITE RIGHT\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_LIGHTNING\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "off_formation_KINGS SPLIT RIGHT \n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "off_formation_KINGS SPLIT LEFT\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "off_formation_KINGS SPLIT LEFT\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_LIGHTNING\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_LIGHTNING TITE LEFT\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "off_formation_LIGHTNING TITE RIGHT\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_KINGS SPLIT LEFT\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "off_formation_KINGS SPLIT RIGHT \n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_LIGHTNING TITE RIGHT\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "off_formation_LIGHTNING TITE LEFT\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "off_formation_KINGS SPLIT LEFT\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "off_formation_KINGS SPLIT RIGHT \n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_LIGHTNING TITE RIGHT\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_LIGHTNING TITE LEFT\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_KINGS SPLIT RIGHT \n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "off_formation_LIGHTNING TITE LEFT\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "off_formation_LIGHTNING TITE LEFT\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "off_formation_KINGS SPLIT LEFT\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_LIGHTNING\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "off_formation_LIGHTNING TITE RIGHT\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_KINGS SPLIT RIGHT \n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "off_formation_LIGHTNING TITE RIGHT\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_KINGS SPLIT RIGHT \n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "off_formation_EMPTY\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_LIGHTNING TITE LEFT\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_EMPTY\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_KINGS SPLIT RIGHT \n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "off_formation_LIGHTNING\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "off_formation_LIGHTNING TITE RIGHT\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_LIGHTNING TITE RIGHT\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "off_formation_KINGS SPLIT LEFT\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_EMPTY\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "off_formation_LIGHTNING TITE LEFT\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_LIGHTNING\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_EMPTY\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "off_formation_KINGS SPLIT LEFT\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "off_formation_LIGHTNING TITE RIGHT\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "off_formation_LIGHTNING\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_LIGHTNING TITE RIGHT\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "off_formation_KINGS SPLIT RIGHT \n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "off_formation_LIGHTNING TITE LEFT\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_KINGS SPLIT LEFT\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_LIGHTNING TITE RIGHT\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_LIGHTNING\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_KINGS SPLIT LEFT\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "off_formation_LIGHTNING TITE RIGHT\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_KINGS SPLIT LEFT\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "off_formation_LIGHTNING TITE RIGHT\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_LIGHTNING TITE RIGHT\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_LIGHTNING TITE RIGHT\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "off_formation_LIGHTNING TITE LEFT\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "off_formation_KINGS SPLIT RIGHT \n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_LIGHTNING TITE RIGHT\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_KINGS SPLIT LEFT\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_EMPTY\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "off_formation_KINGS SPLIT RIGHT \n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "off_formation_LIGHTNING TITE LEFT\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_KINGS SPLIT LEFT\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "off_formation_LIGHTNING TITE LEFT\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_LIGHTNING TITE RIGHT\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "off_formation_LIGHTNING TITE RIGHT\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "off_formation_KINGS SPLIT RIGHT \n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_LIGHTNING TITE LEFT\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "off_formation_LIGHTNING\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "off_formation_LIGHTNING TITE LEFT\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "off_formation_LIGHTNING\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "off_formation_LIGHTNING TITE LEFT\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "off_formation_LIGHTNING TITE LEFT\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "off_formation_LIGHTNING TITE LEFT\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "off_formation_LIGHTNING TITE RIGHT\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "off_formation_EMPTY\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "off_formation_EMPTY\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "off_formation_LIGHTNING TITE RIGHT\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_LIGHTNING TITE RIGHT\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "off_formation_LIGHTNING TITE RIGHT\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_KINGS SPLIT RIGHT \n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "off_formation_LIGHTNING\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_KINGS SPLIT LEFT\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "off_formation_LIGHTNING\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_LIGHTNING\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "off_formation_LIGHTNING TITE LEFT\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "off_formation_LIGHTNING TITE LEFT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
        "\n",
        "# Assuming X_train_scaled is already defined and is a 2D NumPy array\n",
        "n_features = X_train_scaled.shape[1]  # Number of features in your dataset\n",
        "n_players = 22  # Number of players\n",
        "n_coordinates = 2  # XY coordinates\n",
        "n_channels = 1  # Single channel\n",
        "# Reshape the data for Conv1D layer\n",
        "X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], n_players, n_coordinates))\n",
        "X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], n_players, n_coordinates))\n",
        "print(X_train_reshaped.shape)\n",
        "print(X_test_reshaped.shape)\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential([\n",
        "    # First Conv1D layer\n",
        "    Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_players, n_coordinates)),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "\n",
        "    # Additional Conv1D layers (you can add more or modify as needed)\n",
        "    Conv1D(filters=128, kernel_size=2, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "\n",
        "    # Flatten the output before feeding it into dense layers\n",
        "    Flatten(),\n",
        "\n",
        "    # Dense layers for classification\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(y_train.shape[1], activation='softmax')  # Update with your actual number of classes\n",
        "])\n",
        "\n",
        "learning_rate = 0.001  # Experiment with different rates\n",
        "optimizer = SGD(learning_rate=learning_rate, momentum= .8)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# Early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_reshaped, y_train, validation_split=0.2, epochs=400, batch_size=30, callbacks=[early_stopping])\n",
        "test_loss, test_accuracy = model.evaluate(X_test_reshaped, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy}\")"
      ],
      "metadata": {
        "id": "ikmf5__4lmON",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6750b24a-4556-41d7-854e-37508adac3b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(96, 22, 2)\n",
            "(24, 22, 2)\n",
            "Epoch 1/400\n",
            "3/3 [==============================] - 1s 122ms/step - loss: 1.8098 - accuracy: 0.1711 - val_loss: 1.7459 - val_accuracy: 0.3500\n",
            "Epoch 2/400\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 1.8475 - accuracy: 0.1316 - val_loss: 1.7452 - val_accuracy: 0.3500\n",
            "Epoch 3/400\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 1.8081 - accuracy: 0.1711 - val_loss: 1.7449 - val_accuracy: 0.3500\n",
            "Epoch 4/400\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 1.8182 - accuracy: 0.1842 - val_loss: 1.7438 - val_accuracy: 0.3500\n",
            "Epoch 5/400\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 1.7861 - accuracy: 0.1842 - val_loss: 1.7429 - val_accuracy: 0.3000\n",
            "Epoch 6/400\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 1.8067 - accuracy: 0.1711 - val_loss: 1.7424 - val_accuracy: 0.2500\n",
            "Epoch 7/400\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 1.8056 - accuracy: 0.1447 - val_loss: 1.7425 - val_accuracy: 0.2500\n",
            "Epoch 8/400\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 1.7924 - accuracy: 0.1579 - val_loss: 1.7421 - val_accuracy: 0.2500\n",
            "Epoch 9/400\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 1.8294 - accuracy: 0.1711 - val_loss: 1.7422 - val_accuracy: 0.2500\n",
            "Epoch 10/400\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 1.8218 - accuracy: 0.1447 - val_loss: 1.7418 - val_accuracy: 0.2000\n",
            "Epoch 11/400\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 1.7890 - accuracy: 0.1974 - val_loss: 1.7415 - val_accuracy: 0.1500\n",
            "Epoch 12/400\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 1.7902 - accuracy: 0.1974 - val_loss: 1.7411 - val_accuracy: 0.1500\n",
            "Epoch 13/400\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 1.7867 - accuracy: 0.1842 - val_loss: 1.7413 - val_accuracy: 0.1500\n",
            "Epoch 14/400\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 1.7864 - accuracy: 0.1579 - val_loss: 1.7416 - val_accuracy: 0.1500\n",
            "Epoch 15/400\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 1.7708 - accuracy: 0.1711 - val_loss: 1.7417 - val_accuracy: 0.1500\n",
            "Epoch 16/400\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 1.7950 - accuracy: 0.1579 - val_loss: 1.7413 - val_accuracy: 0.1500\n",
            "Epoch 17/400\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 1.7858 - accuracy: 0.1711 - val_loss: 1.7409 - val_accuracy: 0.1500\n",
            "Epoch 18/400\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 1.7550 - accuracy: 0.2632 - val_loss: 1.7410 - val_accuracy: 0.1500\n",
            "Epoch 19/400\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 1.7664 - accuracy: 0.1842 - val_loss: 1.7412 - val_accuracy: 0.1500\n",
            "Epoch 20/400\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 1.7835 - accuracy: 0.1842 - val_loss: 1.7411 - val_accuracy: 0.2000\n",
            "Epoch 21/400\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 1.7393 - accuracy: 0.2368 - val_loss: 1.7410 - val_accuracy: 0.2000\n",
            "Epoch 22/400\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 1.7760 - accuracy: 0.2237 - val_loss: 1.7405 - val_accuracy: 0.2000\n",
            "Epoch 23/400\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 1.7469 - accuracy: 0.2368 - val_loss: 1.7408 - val_accuracy: 0.2000\n",
            "Epoch 24/400\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 1.7652 - accuracy: 0.2632 - val_loss: 1.7409 - val_accuracy: 0.2000\n",
            "Epoch 25/400\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 1.7482 - accuracy: 0.2105 - val_loss: 1.7411 - val_accuracy: 0.1500\n",
            "Epoch 26/400\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 1.7742 - accuracy: 0.1711 - val_loss: 1.7410 - val_accuracy: 0.1500\n",
            "Epoch 27/400\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 1.7654 - accuracy: 0.1974 - val_loss: 1.7409 - val_accuracy: 0.1500\n",
            "Epoch 28/400\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 1.7448 - accuracy: 0.2105 - val_loss: 1.7406 - val_accuracy: 0.1000\n",
            "Epoch 29/400\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 1.7802 - accuracy: 0.1842 - val_loss: 1.7406 - val_accuracy: 0.1000\n",
            "Epoch 30/400\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 1.7367 - accuracy: 0.2895 - val_loss: 1.7405 - val_accuracy: 0.1500\n",
            "Epoch 31/400\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 1.7358 - accuracy: 0.2895 - val_loss: 1.7407 - val_accuracy: 0.1500\n",
            "Epoch 32/400\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 1.7361 - accuracy: 0.3158 - val_loss: 1.7411 - val_accuracy: 0.1500\n",
            "Epoch 33/400\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 1.7446 - accuracy: 0.2500 - val_loss: 1.7412 - val_accuracy: 0.1500\n",
            "Epoch 34/400\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 1.7383 - accuracy: 0.2105 - val_loss: 1.7411 - val_accuracy: 0.1500\n",
            "Epoch 35/400\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 1.7646 - accuracy: 0.1842 - val_loss: 1.7407 - val_accuracy: 0.1500\n",
            "Epoch 36/400\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 1.7285 - accuracy: 0.2500 - val_loss: 1.7411 - val_accuracy: 0.1500\n",
            "Epoch 37/400\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 1.7754 - accuracy: 0.2105 - val_loss: 1.7411 - val_accuracy: 0.1500\n",
            "Epoch 38/400\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 1.7526 - accuracy: 0.2632 - val_loss: 1.7418 - val_accuracy: 0.1500\n",
            "Epoch 39/400\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 1.7441 - accuracy: 0.3158 - val_loss: 1.7420 - val_accuracy: 0.1500\n",
            "Epoch 40/400\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 1.6991 - accuracy: 0.3684 - val_loss: 1.7420 - val_accuracy: 0.1500\n",
            "Epoch 41/400\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 1.6996 - accuracy: 0.3289 - val_loss: 1.7423 - val_accuracy: 0.1500\n",
            "Epoch 42/400\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 1.6940 - accuracy: 0.3158 - val_loss: 1.7423 - val_accuracy: 0.1500\n",
            "Epoch 43/400\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 1.7371 - accuracy: 0.3289 - val_loss: 1.7425 - val_accuracy: 0.1500\n",
            "Epoch 44/400\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 1.7178 - accuracy: 0.3158 - val_loss: 1.7431 - val_accuracy: 0.1500\n",
            "Epoch 45/400\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.7211 - accuracy: 0.2368 - val_loss: 1.7433 - val_accuracy: 0.1500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 1.8011 - accuracy: 0.1250\n",
            "Test Accuracy: 0.125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in X_train_scaled:\n",
        "  new_input_scaled = scaler.transform(i.reshape(1, -1))\n",
        "\n",
        "  # Reshape the data for the model, if necessary\n",
        "  input_data_reshaped = new_input_scaled.reshape((1, 11, 2))\n",
        "  predictions = model.predict(input_data_reshaped)\n",
        "  predicted_class_index = np.argmax(predictions, axis=1)\n",
        "  text = y.columns[predicted_class_index][0]  # G\n",
        "  print(text)"
      ],
      "metadata": {
        "id": "C1S3TGIdPQfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving and exporting the model"
      ],
      "metadata": {
        "id": "vZhLe-iEppWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the scaler\n",
        "joblib.dump(scaler, 'my_scaler.pkl')\n",
        "model.save('my_model.h5')  # Save as HDF5\n",
        "# or\n",
        "model.save('my_saved_model')  # Save as SavedModel"
      ],
      "metadata": {
        "id": "LtYhYkfyoAiD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20b7fe53-265f-4b33-a261-a8775c64592f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    }
  ]
}